{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 19/Set até às 23:59.<br />\n",
    "Grupo: 2 ou 3 pessoas - grupos com 3 pessoas terá uma rubrica diferenciada.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO gravar a key do professor no arquivo**\n",
    "\n",
    "\n",
    "### Entrega Intermediária: Check 1 - APS 2\n",
    "\n",
    "Até o dia 10/Set às 23:59, xlsx deve estar no Github com as seguintes evidências: \n",
    "\n",
    "  * Produto escolhido.\n",
    "  * Arquivo Excel contendo a base de treinamento e a base de testes já classificadas.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Parte I - Adquirindo a Base de Dados\n",
    "\n",
    "Acessar o notebook **Projeto-2-Planilha** para realizar a coleta dos dados. O grupo deve classificar os dados coletados manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Parte II - Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Não se esqueça de implementar o Laplace Smoothing (https://en.wikipedia.org/wiki/Laplace_smoothing).\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n",
    "Escreva o seu código abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "### André E. Weber e Michel José Moraes\n",
    "\n",
    "Decidimos usar o termo Yakult como nosso experimento para a elaboração e o teste do algorítmo de Naive-Bayes por conta de ser um tema comentado frequentemente pelos brasileiros pelo sabor, cultura, tradição entre os jovens e uma bebida frequentemente feita em casa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renomeando categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_excel('tweets.xlsx') #Planilha Treinamento definida\n",
    "tweets[\"Categoria\"] = tweets[\"Categoria\"].replace(0,\"Irrelevante\")\n",
    "tweets[\"Categoria\"] = tweets[\"Categoria\"].replace(2,\"Relevante\")\n",
    "tweets[\"Categoria\"] = tweets[\"Categoria\"].replace(1,\"Relevante\")\n",
    "\n",
    "\n",
    "\n",
    "tweets_teste= pd.read_excel(\"tweets.xlsx\",sheet_name=1) #Planilha Teste definida\n",
    "tweets_teste[\"Categoria\"] = tweets_teste[\"Categoria\"].replace(0,\"Irrelevante\")\n",
    "tweets_teste[\"Categoria\"] = tweets_teste[\"Categoria\"].replace(2,\"Relevante\")\n",
    "tweets_teste[\"Categoria\"] = tweets_teste[\"Categoria\"].replace(1,\"Relevante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que limpa a coluna do dataframe de uma vez\n",
    "def limpeza(tweets, coluna):\n",
    "    tweets[coluna] = tweets[coluna].apply(lambda x: x.lower())\n",
    "\n",
    "    for frase in tweets[coluna]:\n",
    "        frase_split = frase.split(\" \")\n",
    "        for palavra in frase_split:\n",
    "            if len(palavra) == 1:\n",
    "                frase_split.remove(palavra)\n",
    "\n",
    "\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('portuguese')\n",
    "    stop_list = [\"?\", \"de\", \"é\", \"á\", \"à\", \"ao\", \"a\", \"o\", \"é\", \"rt\", '\"', \"“\", \"'\", \",\", \":\", \".\", \"(\", \")\", \"!\", \"$\", \"%\", \"*\", \"&\", \"-\", \"+\", \"=\", \"/\"]\n",
    "    stop.extend(stop_list)\n",
    "    tweets[coluna] = [' '.join([w for w in x.lower().split() if w not in stop]) \n",
    "        for x in tweets[coluna].tolist()]\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"  \",\" \")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\",\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"?\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"'\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace('\"',\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"“\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\":\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\".\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\")\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"(\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"!\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"$\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"%\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"*\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"&\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"-\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"+\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"=\",\"\")\n",
    "    tweets[coluna] = tweets[coluna].str.replace(\"/\",\"\")\n",
    "    return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-483-d43e0f96e658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimpeza\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Treinamento\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtweets_irrel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Treinamento'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Categoria'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Irrelevante\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtweets_rel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Treinamento'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Categoria'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Relevante\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-482-93186ad607e9>\u001b[0m in \u001b[0;36mlimpeza\u001b[1;34m(tweets, coluna)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"@\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"https\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpalavra\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoluna\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcoluna\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "tweets = limpeza(tweets, \"Treinamento\")\n",
    "\n",
    "tweets_irrel=tweets['Treinamento'][tweets['Categoria'] == \"Irrelevante\"]\n",
    "tweets_rel=tweets['Treinamento'][tweets['Categoria'] == \"Relevante\"]\n",
    "\n",
    "tweets_irrel=pd.DataFrame(tweets_irrel)\n",
    "tweets_rel=pd.DataFrame(tweets_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Obrigatório para grupos de 3 alunos:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contando tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irrel_total = tweets['Treinamento'][tweets['Categoria'] == \"Irrelevante\"].count() #Contagem do total de irrelevantes calssificados\n",
    "rel_total = tweets['Treinamento'][tweets['Categoria'] == \"Relevante\"].count() #Contagem do total de relevantes calssificados\n",
    "total = tweets[\"Treinamento\"].count() #Contagem total dos tweets\n",
    "\n",
    "porcent_rel= (rel_total/total) #Probabilidade de relevantes\n",
    "porcent_irrel= (irrel_total/total) #Probabilidade de irrelevantes\n",
    "\n",
    "print (\"Porcentagem de relevantes:{0}%\".format(porcent_rel*100))\n",
    "print (\"Porcentagem de irrelevantes:{0}%\".format(porcent_irrel*100))\n",
    "\n",
    "print (\"Total de relevantes:{0}\".format(rel_total))\n",
    "print (\"Total de irrelevantes:{0}\".format(irrel_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contando aparições palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_total = {} #Dicionario para o total de palavras e ocorrências\n",
    "palavras_rel={} #Dicionario para o total de palavras relevantes e suas ocorrências\n",
    "palavras_irrel={} #Dicionario para o total de palavras irrelevantes e suas ocorrências\n",
    "\n",
    "#Relevantes\n",
    "for frase in tweets_rel[\"Treinamento\"]:\n",
    "    for palavra in frase.split():\n",
    "        if palavra not in palavras_rel:\n",
    "            palavras_rel[palavra]=1\n",
    "        else:\n",
    "            palavras_rel[palavra]+=1\n",
    "            \n",
    "#Irrelevantes\n",
    "for frase in tweets_irrel[\"Treinamento\"]:\n",
    "    for palavra in frase.split():\n",
    "        if palavra not in palavras_irrel:\n",
    "            palavras_irrel[palavra]=1\n",
    "        else:\n",
    "            palavras_irrel[palavra]+=1\n",
    "#Total\n",
    "for frase in tweets[\"Treinamento\"]:\n",
    "    for palavra in frase.split():\n",
    "        if palavra not in palavras_total:\n",
    "            palavras_total[palavra]=1\n",
    "        else:\n",
    "            palavras_total[palavra]+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contador probabilidade palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que cálcula probabilidade de cada palavra e frase a ser relevante ou irrelevante, retornando a mais provável das opções\n",
    "def prob_palavra(frase, dic1, dic2):\n",
    "    prob_irrel = porcent_irrel\n",
    "    prob_rel = porcent_rel\n",
    "    frase_split = frase.split()\n",
    "    soma_irrel = sum(palavras_irrel.values())\n",
    "    soma_rel = sum(palavras_rel.values())\n",
    "    resultado=str()\n",
    "    for palavra in frase_split:\n",
    "        quantidade_rel = 1\n",
    "        quantidade_irrel = 1\n",
    "        if palavra in dic1:\n",
    "            quantidade_rel += palavras_rel[palavra]\n",
    "        if palavra in dic2:\n",
    "            quantidade_irrel += palavras_irrel[palavra]\n",
    "        prob_rel *= (quantidade_rel)/(soma_irrel + len(palavras_total))\n",
    "        prob_irrel *= (quantidade_irrel)/(soma_rel + len(palavras_total))\n",
    "    if prob_irrel > prob_rel:\n",
    "        resultado = \"Irrelevante\"\n",
    "    else:\n",
    "        resultado = \"Relevante\"\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = input(\"Digite uma frase: \")\n",
    "prob_palavra(frase, palavras_rel, palavras_irrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar a porcentagem de acerto do classificador (precisão)\n",
    "previsao = []\n",
    "for frase in tweets_teste[\"Teste\"]:\n",
    "    previsao.append(prob_palavra(frase, palavras_rel, palavras_irrel))\n",
    "tweets_teste[\"Previsão\"] = previsao\n",
    "tweets_teste\n",
    "\n",
    "numero_acerto = tweets_teste[\"Teste\"][tweets_teste[\"Categoria\"] == tweets_teste[\"Previsão\"]].count()\n",
    "numero_acerto\n",
    "\n",
    "print(numero_acerto/len(tweets_teste) * 100,\"% de acerto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "\n",
    "&nbsp; O classificador demonstrou um acerto de 68,5%. O resultado não é alto e sim razoável, dessa forma poderia ser usado para apenas uma colaboração de pesquisas ou um embasamento para futuras investigações.\n",
    "\n",
    "&nbsp; A conclusão que pode ser tirada a partir da classificação, foi de que houveram muitos \"tweets\" acerca de uma novidade. Um filme lançado pelo netflix, uma plataforma que vem ganhando cada vez mais notoriedade, em 17 de agosto de 2018. Em uma das cenas, um dos personagens utiliza uma bebida parecida com \"Yakult\" para elaborar um drink alcoólico, o que gerou muita repercussão entre os americanos se perguntando o que seria isso, levando-os a experimentar, gerando diversos comentários. Consequentemente os brasileiros se sentiram surpresos ao ver a famosa bebida sendo exibida internacionalmente e ao mesmo tempo por conta da novidade que foi para os estrangeiros. \n",
    "&nbsp; Dessa forma, houve uma grande quantidade de tweets irrelevantes, enquanto outros o contrário, porém levando mais em conta do próprio filme.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp; Nosso classificador se baseia em relevância, então poderíamos incrementar um método para positivos e negativos, dessa forma também identificar ironias e sarcasmos vistos durante a pesquisa, como \"Yalkut é bom sim, por isso da diarreia\".\n",
    "\n",
    "&nbsp; O projeto seria algo de bom financiamento por conta do foco em algo com bom desempenho, limpeza para dados mais puros e rápidos e simplicidade do código. Fatores que giram em torno de um bom desempenho.\n",
    "\n",
    "&nbsp; Utilizar os tweets classificados para alimentar a base de dados seria uma boa ideia, porém é inviável devido a margem de erro alta, assim poderia manipular os dados erroneamente.\n",
    "\n",
    "&nbsp; O decorrer da elaboração do projeto nos fez chegar mais a fundo sobre o algorítmo de Naive-Bayes, por conta de sua possível expansão e se movendo entre polos complexos ou simples. Alguns exemplos de utilização são, identificação de spam, classificação de artigos através de categorias, reconhecimento facial e também diagnóstico médico, como a detecção de H1N1 entre os pacientes, com positivo correto de 98% e negativo de 97%.\n",
    "\n",
    "\n",
    "&nbsp; Para o melhoramento do classificador, poderia aumentar a base de treinamento, uma melhor classificação de relevância manual para gerar menor taxa de erros. Uma possibilidade também, seria o melhoramento da performance através de alguns estudos feitos sobre o relaxamento da suposição de independência (Melhorando a Performance do Algoritmo Naive Bayes para Regressão Através da Combinação de Atributos, ResearchGate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
